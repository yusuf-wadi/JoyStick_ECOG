{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook we will be trying the transformer architecture\n",
    "This choice is motivated by the fact that the transformer excels at seqtoseq, and is able to capture dependencies spatially and temporally. I hypothesize that this is a good fit for the task since ECOG data is very complex and has dependencies in both time and space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import FastICA\n",
    "from scipy.stats import kurtosis\n",
    "from sklearn.decomposition import PCA\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import math\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_vectors(coords, resolution):\n",
    "    \"\"\"\n",
    "    Calculate the average vectors of x and y magnitudes given a resolution.\n",
    "\n",
    "    Parameters:\n",
    "    coords (list of tuples): An array of (x, y) coordinates.\n",
    "    resolution (int): The resolution number.\n",
    "\n",
    "    Returns:\n",
    "    list of tuples: An array of vectors of x and y magnitudes.\n",
    "    \"\"\"\n",
    "    if resolution <= 0:\n",
    "        raise ValueError(\"Resolution must be a positive integer.\")\n",
    "    \n",
    "    n = len(coords)\n",
    "    if resolution > n:\n",
    "        raise ValueError(\"Resolution cannot be greater than the length of the coordinates array.\")\n",
    "    \n",
    "    # Calculate the segment length\n",
    "    segment_length = n // resolution\n",
    "    \n",
    "    vectors = []\n",
    "    \n",
    "    for i in range(0, n, segment_length):\n",
    "        segment = coords[i:i + segment_length]\n",
    "        if len(segment) < segment_length:\n",
    "            break\n",
    "        \n",
    "        # Calculate the average change in x and y\n",
    "        delta_x = np.mean([segment[j+1][0] - segment[j][0] for j in range(len(segment) - 1)])\n",
    "        delta_y = np.mean([segment[j+1][1] - segment[j][1] for j in range(len(segment) - 1)])\n",
    "        \n",
    "        vectors.append((delta_x, delta_y))\n",
    "    \n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_joystick_readings(x, y, resolution):\n",
    "     return calculate_vectors(list(zip(x, y)), resolution)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ica(X, n_components):\n",
    "    ica = FastICA(n_components=n_components)\n",
    "    S_ = ica.fit_transform(X)  # Reconstruct signals\n",
    "    A_ = ica.mixing_  # Get estimated mixing matrix\n",
    "    return S_, A_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(d1):\n",
    "    scaler = StandardScaler()\n",
    "    d1 = scaler.fit_transform(d1)\n",
    "    return d1\n",
    "\n",
    "def train_test_split(X, y, train_size):\n",
    "    train_size = int(train_size * len(X))\n",
    "    return X[:train_size], X[train_size:], y[:train_size], y[train_size:]\n",
    "\n",
    "def chunk_data(data, length):\n",
    "    chunk_size = length\n",
    "    return [data[i:i+chunk_size] for i in range(0, len(data), len(data)//chunk_size) if i+chunk_size <= len(data)]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_process_joystick_data():\n",
    "    # Load data from file\n",
    "    fname = './data/joystick_track.npz'\n",
    "    alldat = np.load(fname, allow_pickle=True)['dat']\n",
    "    dat = alldat[0]\n",
    "    patient_idx = 1\n",
    "    d = dat[patient_idx]\n",
    "    \n",
    "    # Extract ECoG data and joystick vectors\n",
    "    ecog_data = d['V']\n",
    "    targetX = d['targetX']\n",
    "    targetY = d['targetY']\n",
    "    \n",
    "    print(ecog_data.shape, targetX.shape, targetY.shape )\n",
    "    # Normalize and PCA\n",
    "    ecog_data = normalize_data(ecog_data)\n",
    "    ecog_data = PCA(n_components=0.95).fit_transform(ecog_data)\n",
    "    print(ecog_data.shape, targetX.shape, targetY.shape )\n",
    "    # Set resolution and vectorize joystick readings\n",
    "    resolution = 700\n",
    "    targetX, targetY = normalize_data(targetX), normalize_data(targetY)\n",
    "    vectors = np.array(vectorize_joystick_readings(targetX, targetY, resolution))\n",
    "    print(ecog_data.shape, vectors.shape)\n",
    "    # Chunk ECoG data based on resolution\n",
    "    chunked_ecog_data = chunk_data(ecog_data, len(vectors))\n",
    "    # Remove data points with zero vectors\n",
    "    ecog_data, vectors = zip(*[(x, y) for x, y in zip(ecog_data, vectors) if not np.array_equal(y, [0, 0])])\n",
    "    print(len(ecog_data), len(ecog_data[0]), len(vectors))\n",
    "    print(len(chunked_ecog_data), len(chunked_ecog_data[0]), len(chunked_ecog_data[0][0]))\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(chunked_ecog_data, vectors, train_size=0.8)\n",
    "    \n",
    "    # Convert data back to numpy arrays\n",
    "    X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "    X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(248640, 64) (248640, 1) (248640, 1)\n",
      "(248640, 28) (248640, 1) (248640, 1)\n",
      "(248640, 28) (700, 2)\n",
      "679 28 679\n",
      "699 700 28\n",
      "(559, 700, 28) (559, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = load_process_joystick_data()\n",
    "min_len = min(len(X_test), len(y_test))\n",
    "X_test = X_test[:min_len]\n",
    "y_test = y_test[:min_len]\n",
    "# reshape input to be [time steps, batch size, features]\n",
    "\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfromers: robots in disguise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "torch.set_default_device('cuda') if torch.cuda.is_available() else torch.set_default_device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32, requires_grad=True)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32, requires_grad=True)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([559, 700, 28]) torch.Size([120, 700, 28]) torch.Size([559, 2]) torch.Size([120, 2])\n",
      "tensor([[[-3.5636e+00, -2.2142e-01,  3.5455e-01,  ..., -4.0250e-01,\n",
      "           3.7540e-01, -4.1878e-01],\n",
      "         [-4.2145e+00, -2.3734e-01,  3.7031e-01,  ..., -2.9681e-01,\n",
      "           3.7048e-01, -3.8401e-01],\n",
      "         [-4.5517e+00, -2.5918e-01,  3.8189e-01,  ..., -1.7190e-01,\n",
      "           3.6903e-01, -3.3763e-01],\n",
      "         ...,\n",
      "         [ 6.4868e+00,  1.6992e+00, -2.0743e+00,  ...,  5.2482e-01,\n",
      "          -1.9219e-01, -2.8105e-01],\n",
      "         [ 6.2328e+00,  1.7402e+00, -2.0340e+00,  ...,  5.7298e-01,\n",
      "          -2.0215e-01, -3.0032e-01],\n",
      "         [ 6.2450e+00,  1.7968e+00, -1.9920e+00,  ...,  6.2103e-01,\n",
      "          -2.0591e-01, -3.0314e-01]],\n",
      "\n",
      "        [[-3.5565e+00, -4.3673e-01,  4.1906e-01,  ...,  4.6836e-02,\n",
      "          -7.1411e-02,  2.5605e-01],\n",
      "         [-2.9171e+00, -3.2429e-01,  3.5420e-01,  ...,  4.7743e-02,\n",
      "          -1.0004e-01,  2.7177e-01],\n",
      "         [-2.2750e+00, -2.2674e-01,  2.8591e-01,  ...,  3.9488e-02,\n",
      "          -1.1557e-01,  2.9023e-01],\n",
      "         ...,\n",
      "         [-8.1919e+00,  2.2611e+00, -8.7913e-02,  ...,  8.3881e-02,\n",
      "           3.3436e-02, -1.5160e-01],\n",
      "         [-7.9328e+00,  2.2634e+00, -1.5097e-01,  ...,  1.0792e-01,\n",
      "          -9.0515e-04, -1.2958e-01],\n",
      "         [-8.1317e+00,  2.2455e+00, -2.0338e-01,  ...,  1.2999e-01,\n",
      "          -2.8658e-02, -1.2399e-01]],\n",
      "\n",
      "        [[ 4.5927e+00,  2.1069e+00, -2.0275e+00,  ...,  6.8268e-01,\n",
      "          -2.4303e-01, -3.0857e-02],\n",
      "         [ 4.0727e+00,  2.0947e+00, -2.0418e+00,  ...,  6.7088e-01,\n",
      "          -2.2068e-01, -3.4908e-02],\n",
      "         [ 3.2212e+00,  2.0746e+00, -2.0460e+00,  ...,  6.6235e-01,\n",
      "          -1.8689e-01, -4.5294e-02],\n",
      "         ...,\n",
      "         [ 8.6765e-01,  3.2263e+00,  2.4916e+00,  ...,  6.3032e-01,\n",
      "           9.0620e-01,  4.5329e-01],\n",
      "         [ 5.5706e-01,  3.1985e+00,  2.4542e+00,  ...,  6.6576e-01,\n",
      "           9.1491e-01,  4.2369e-01],\n",
      "         [ 6.0026e-01,  3.2160e+00,  2.3966e+00,  ...,  6.8075e-01,\n",
      "           9.1655e-01,  3.8583e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-2.4526e+00, -4.9559e-01,  5.7188e-01,  ..., -2.8210e-01,\n",
      "          -1.1825e-01,  1.0276e-01],\n",
      "         [-2.4661e+00, -4.1688e-01,  5.9906e-01,  ..., -2.2449e-01,\n",
      "          -1.4074e-01,  8.5657e-02],\n",
      "         [-2.2135e+00, -3.8908e-01,  6.2213e-01,  ..., -2.1468e-01,\n",
      "          -1.4543e-01,  7.8639e-02],\n",
      "         ...,\n",
      "         [ 1.0593e+00,  1.4805e-01,  1.1763e+00,  ...,  3.2582e-01,\n",
      "           1.9662e-01, -1.7005e-01],\n",
      "         [ 1.5309e+00,  2.8464e-01,  1.2454e+00,  ...,  3.5524e-01,\n",
      "           1.5091e-01, -1.6824e-01],\n",
      "         [ 2.0146e+00,  2.7610e-01,  1.3344e+00,  ...,  3.6980e-01,\n",
      "           1.0034e-01, -2.0764e-01]],\n",
      "\n",
      "        [[-3.2386e+00,  2.2897e+00,  4.0329e-01,  ...,  9.0068e-02,\n",
      "           3.1288e-01, -1.7265e-01],\n",
      "         [-3.5712e+00,  2.1773e+00,  3.8234e-01,  ...,  1.2687e-01,\n",
      "           3.0366e-01, -1.9239e-01],\n",
      "         [-3.8136e+00,  2.1366e+00,  3.5033e-01,  ...,  2.0125e-01,\n",
      "           2.8217e-01, -2.0910e-01],\n",
      "         ...,\n",
      "         [-4.2710e+00, -2.4683e-01,  2.6999e-01,  ..., -4.4084e-01,\n",
      "          -5.6984e-01,  4.0109e-01],\n",
      "         [-4.4607e+00, -3.0995e-01,  2.4516e-01,  ..., -4.6207e-01,\n",
      "          -5.5532e-01,  3.4892e-01],\n",
      "         [-4.6675e+00, -2.7475e-01,  1.8767e-01,  ..., -4.4712e-01,\n",
      "          -5.5150e-01,  3.1155e-01]],\n",
      "\n",
      "        [[ 3.2530e+00,  1.8002e-01,  2.4690e+00,  ...,  5.7238e-01,\n",
      "          -3.4112e-01, -3.2738e-01],\n",
      "         [ 3.4571e+00,  2.4562e-01,  2.5161e+00,  ...,  6.1124e-01,\n",
      "          -3.9225e-01, -3.3808e-01],\n",
      "         [ 3.7832e+00,  2.6237e-01,  2.5713e+00,  ...,  6.3137e-01,\n",
      "          -4.3854e-01, -3.4606e-01],\n",
      "         ...,\n",
      "         [-6.7627e+00, -8.3022e-01,  2.0429e-01,  ...,  1.7756e-01,\n",
      "          -4.1441e-01, -4.8213e-01],\n",
      "         [-6.3488e+00, -7.0491e-01,  1.6333e-01,  ...,  2.0893e-01,\n",
      "          -4.5050e-01, -5.0064e-01],\n",
      "         [-5.9837e+00, -7.4077e-01,  2.2094e-01,  ...,  2.0694e-01,\n",
      "          -4.4689e-01, -5.4824e-01]]], device='cuda:0', requires_grad=True) tensor([[ 0.0077,  0.0007],\n",
      "        [-0.0005,  0.0012],\n",
      "        [-0.0009,  0.0009],\n",
      "        ...,\n",
      "        [ 0.0008, -0.0012],\n",
      "        [ 0.0011, -0.0008],\n",
      "        [ 0.0014, -0.0004]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#print all shapes\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "print(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=6000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.encoding[:, :x.size(1), :]\n",
    "\n",
    "class ECoGTransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, model_dim, num_heads, num_layers, output_dim, dropout_rate=0.1):\n",
    "        super(ECoGTransformerEncoder, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, model_dim)\n",
    "        self.pos_encoder = PositionalEncoding(model_dim)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=model_dim, nhead=num_heads, dropout=dropout_rate)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.fc = nn.Linear(model_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = checkpoint(self.transformer_encoder, x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.fc(x[:, -1, :])\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new loss\n",
    "class ScaledMSELoss(nn.Module):\n",
    "    def __init__(self, scale_factor):\n",
    "        super(ScaledMSELoss, self).__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        return self.mse_loss(input, target) * self.scale_factor\n",
    "\n",
    "# Example usage\n",
    "scale_factor = 1000  # Adjust the scale factor as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thewa\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "# Example hyperparameters\n",
    "input_dim = X_train.shape[-1]\n",
    "model_dim = 512\n",
    "num_heads = 16\n",
    "num_layers = 8\n",
    "output_dim = 2 \n",
    "dropout_rate = 0.3670812456459453\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-5\n",
    "\n",
    "# Initialize the model\n",
    "model = ECoGTransformerEncoder(input_dim, model_dim, num_heads, num_layers, output_dim)\n",
    "# Ensure model parameters require gradients\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "# Apply weight initialization\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "# Training loop with mixed precision and gradient checkpointing\n",
    "scaler = GradScaler()\n",
    "criterion = ScaledMSELoss(scale_factor=scale_factor)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "num_epochs = 1000\n",
    "scheduler = StepLR(optimizer, step_size=num_epochs//3, gamma=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'model_dim': 128, 'num_heads': 8, 'num_layers': 4, 'dropout_rate': 0.3670812456459453, 'learning_rate': 0.0006976310761919828"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real data\n",
    "\n",
    "input_tensor = X_train\n",
    "target_tensor = y_train\n",
    "dataset = TensorDataset(input_tensor, target_tensor)\n",
    "generator = torch.Generator(device='cuda')\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True, generator=generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input mean: 0.0007466294919140637, Input std: 1.4338657855987549\n",
      "Target mean: -4.670707483001024e-07, Target std: 0.0010870033875107765\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input mean: {input_tensor.mean()}, Input std: {input_tensor.std()}\")\n",
    "print(f\"Target mean: {target_tensor.mean()}, Target std: {target_tensor.std()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([559, 700, 28]) torch.Size([559, 2])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "train = False\n",
    "losses = []\n",
    "# Early stopping parameters\n",
    "patience = 20\n",
    "best_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "if train:\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for data, target in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                losses.append(loss.item())\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            #scheduler.step()\n",
    "         # Validation step\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in dataloader:\n",
    "                with autocast():\n",
    "                    output = model(data)\n",
    "                    loss = criterion(output, target)\n",
    "                    val_loss += loss.item()\n",
    "        val_loss /= len(dataloader)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss}')\n",
    "        if epoch%10 == 0:\n",
    "            # loss direction as a magnitude of the last 10 losses\n",
    "            print(f'Loss direction: {np.mean(losses[-10:]) - np.mean(losses[-20:-10])}')\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f'\\n\\n---------------------------\\n---------------------------\\nEarly stopping at epoch {epoch+1}\\n---------------------------\\n---------------------------\\n')\n",
    "                break\n",
    "\n",
    "    \n",
    "    torch.save(model.state_dict(), 'transformer_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_loss(losses):\n",
    "    plt.plot(losses)\n",
    "    plt.title('Loss over epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "plot_loss(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ECoGTransformerEncoder:\n\tMissing key(s) in state_dict: \"transformer_encoder.layers.5.self_attn.in_proj_weight\", \"transformer_encoder.layers.5.self_attn.in_proj_bias\", \"transformer_encoder.layers.5.self_attn.out_proj.weight\", \"transformer_encoder.layers.5.self_attn.out_proj.bias\", \"transformer_encoder.layers.5.linear1.weight\", \"transformer_encoder.layers.5.linear1.bias\", \"transformer_encoder.layers.5.linear2.weight\", \"transformer_encoder.layers.5.linear2.bias\", \"transformer_encoder.layers.5.norm1.weight\", \"transformer_encoder.layers.5.norm1.bias\", \"transformer_encoder.layers.5.norm2.weight\", \"transformer_encoder.layers.5.norm2.bias\", \"transformer_encoder.layers.6.self_attn.in_proj_weight\", \"transformer_encoder.layers.6.self_attn.in_proj_bias\", \"transformer_encoder.layers.6.self_attn.out_proj.weight\", \"transformer_encoder.layers.6.self_attn.out_proj.bias\", \"transformer_encoder.layers.6.linear1.weight\", \"transformer_encoder.layers.6.linear1.bias\", \"transformer_encoder.layers.6.linear2.weight\", \"transformer_encoder.layers.6.linear2.bias\", \"transformer_encoder.layers.6.norm1.weight\", \"transformer_encoder.layers.6.norm1.bias\", \"transformer_encoder.layers.6.norm2.weight\", \"transformer_encoder.layers.6.norm2.bias\", \"transformer_encoder.layers.7.self_attn.in_proj_weight\", \"transformer_encoder.layers.7.self_attn.in_proj_bias\", \"transformer_encoder.layers.7.self_attn.out_proj.weight\", \"transformer_encoder.layers.7.self_attn.out_proj.bias\", \"transformer_encoder.layers.7.linear1.weight\", \"transformer_encoder.layers.7.linear1.bias\", \"transformer_encoder.layers.7.linear2.weight\", \"transformer_encoder.layers.7.linear2.bias\", \"transformer_encoder.layers.7.norm1.weight\", \"transformer_encoder.layers.7.norm1.bias\", \"transformer_encoder.layers.7.norm2.weight\", \"transformer_encoder.layers.7.norm2.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# test model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtransformer_model.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[1;32mc:\\Users\\thewa\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2189\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2184\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2185\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2186\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2190\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ECoGTransformerEncoder:\n\tMissing key(s) in state_dict: \"transformer_encoder.layers.5.self_attn.in_proj_weight\", \"transformer_encoder.layers.5.self_attn.in_proj_bias\", \"transformer_encoder.layers.5.self_attn.out_proj.weight\", \"transformer_encoder.layers.5.self_attn.out_proj.bias\", \"transformer_encoder.layers.5.linear1.weight\", \"transformer_encoder.layers.5.linear1.bias\", \"transformer_encoder.layers.5.linear2.weight\", \"transformer_encoder.layers.5.linear2.bias\", \"transformer_encoder.layers.5.norm1.weight\", \"transformer_encoder.layers.5.norm1.bias\", \"transformer_encoder.layers.5.norm2.weight\", \"transformer_encoder.layers.5.norm2.bias\", \"transformer_encoder.layers.6.self_attn.in_proj_weight\", \"transformer_encoder.layers.6.self_attn.in_proj_bias\", \"transformer_encoder.layers.6.self_attn.out_proj.weight\", \"transformer_encoder.layers.6.self_attn.out_proj.bias\", \"transformer_encoder.layers.6.linear1.weight\", \"transformer_encoder.layers.6.linear1.bias\", \"transformer_encoder.layers.6.linear2.weight\", \"transformer_encoder.layers.6.linear2.bias\", \"transformer_encoder.layers.6.norm1.weight\", \"transformer_encoder.layers.6.norm1.bias\", \"transformer_encoder.layers.6.norm2.weight\", \"transformer_encoder.layers.6.norm2.bias\", \"transformer_encoder.layers.7.self_attn.in_proj_weight\", \"transformer_encoder.layers.7.self_attn.in_proj_bias\", \"transformer_encoder.layers.7.self_attn.out_proj.weight\", \"transformer_encoder.layers.7.self_attn.out_proj.bias\", \"transformer_encoder.layers.7.linear1.weight\", \"transformer_encoder.layers.7.linear1.bias\", \"transformer_encoder.layers.7.linear2.weight\", \"transformer_encoder.layers.7.linear2.bias\", \"transformer_encoder.layers.7.norm1.weight\", \"transformer_encoder.layers.7.norm1.bias\", \"transformer_encoder.layers.7.norm2.weight\", \"transformer_encoder.layers.7.norm2.bias\". "
     ]
    }
   ],
   "source": [
    "# test model\n",
    "model.load_state_dict(torch.load('transformer_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test)\n",
    "    loss = criterion(y_pred, y_test)\n",
    "    print(f'Test Loss: {loss.item()}')\n",
    "    y_test = y_test.detach().cpu().numpy()\n",
    "    y_pred = y_pred.detach().cpu().numpy()\n",
    "    # Plot the first 100 predictions\n",
    "    plt.plot(y_test[:100, 0], label='True X')\n",
    "    plt.plot(y_pred[:100, 0], label='Predicted X')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(y_test[:100, 1], label='True Y')\n",
    "    plt.plot(y_pred[:100, 1], label='Predicted Y')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna, the hyperparameter optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-23 12:34:41,322] A new study created in memory with name: no-name-06f5c844-abcc-404d-9567-db42eaa64bb5\n",
      "C:\\Users\\thewa\\AppData\\Local\\Temp\\ipykernel_19348\\1332823889.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-3)\n",
      "C:\\Users\\thewa\\AppData\\Local\\Temp\\ipykernel_19348\\1332823889.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Loss: 410.1645202636719\n",
      "Epoch 2/1000, Loss: 634.5203247070312\n",
      "Epoch 3/1000, Loss: 500.87969970703125\n",
      "Epoch 4/1000, Loss: 204.18307495117188\n",
      "Epoch 5/1000, Loss: 269.0952453613281\n",
      "Epoch 6/1000, Loss: 175.4502410888672\n",
      "Epoch 7/1000, Loss: 201.57382202148438\n",
      "Epoch 8/1000, Loss: 202.6818084716797\n",
      "Epoch 9/1000, Loss: 109.95153045654297\n",
      "Epoch 10/1000, Loss: 127.13128662109375\n",
      "Epoch 11/1000, Loss: 69.03248596191406\n",
      "Epoch 12/1000, Loss: 90.01192474365234\n",
      "Epoch 13/1000, Loss: 134.20545959472656\n",
      "Epoch 14/1000, Loss: 71.65036010742188\n",
      "Epoch 15/1000, Loss: 57.88997268676758\n",
      "Epoch 16/1000, Loss: 47.3446044921875\n",
      "Epoch 17/1000, Loss: 52.556304931640625\n",
      "Epoch 18/1000, Loss: 39.47270202636719\n",
      "Epoch 19/1000, Loss: 34.48288345336914\n"
     ]
    }
   ],
   "source": [
    "# run optuna?\n",
    "run_optuna = True\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Hyperparameters to tune\n",
    "    model_dim = trial.suggest_categorical('model_dim', [128, 256, 512])\n",
    "    num_heads = trial.suggest_categorical('num_heads', [4, 8, 16])\n",
    "    num_layers = trial.suggest_int('num_layers', 4,8)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-3)\n",
    "    weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-4)\n",
    "    \n",
    "    # real data\n",
    "    input_tensor = X_train\n",
    "    target_tensor = y_train\n",
    "    dataset = TensorDataset(input_tensor, target_tensor)\n",
    "    generator = torch.Generator(device='cuda')\n",
    "    dataloader = DataLoader(dataset, batch_size=16, shuffle=True, generator=generator)\n",
    "    \n",
    "    # Model and optimizer\n",
    "    model = ECoGTransformerEncoder(input_dim=X_train.shape[-1], model_dim=model_dim, num_heads=num_heads, num_layers=num_layers, output_dim=2, dropout_rate=dropout_rate)\n",
    "    model.apply(init_weights)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    criterion = ScaledMSELoss(1000)\n",
    "    # Training loop\n",
    "    print(trial.params)\n",
    "    losses = []\n",
    "    for epoch in range(100):\n",
    "        for data, target in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                losses.append(loss.item())\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            #scheduler.step()\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "    general_loss = np.mean(losses[-10:])\n",
    "    return general_loss\n",
    "\n",
    "if run_optuna:\n",
    "    # Run the optimization\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=50)\n",
    "\n",
    "    # Best hyperparameters\n",
    "    print(study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best trial with output dim 1: [I 2024-07-22 11:58:14,634] Trial 15 finished with value: 753723.5625 and parameters: {'model_dim': 256, 'num_heads': 4, 'num_layers': 5, 'dropout_rate': 0.2259481444219587, 'learning_rate': 0.0009022268315380955}. Best is trial 15 with value: 753723.5625.\n",
    "\n",
    "wrong output dim, trying 2.\n",
    "best: {'model_dim': 512, 'num_heads': 8, 'num_layers': 6, 'dropout_rate': 0.48411131648232686, 'learning_rate': 0.0008203239127338236}\n",
    "\n",
    "new optimization.\n",
    "best: [I 2024-07-23 02:32:30,156] Trial 48 finished with value: 372409.4375 and parameters: {'model_dim': 128, 'num_heads': 8, 'num_layers': 4, 'dropout_rate': 0.3670812456459453, 'learning_rate': 0.0006976310761919828}. Best is trial 48 with value: 372409.4375.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
